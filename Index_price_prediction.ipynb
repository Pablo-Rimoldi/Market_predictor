{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "11b9d177280f409387cc9bd8b21b158f",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Financial Data processing for stock analysis\n",
    "\n",
    "Stock market prediction has long been an intriguing subject for researchers across various fields. Some attempt to forecast stock prices by identifying technical patterns, others analyze news sentiment, while some simply replicate the strategies of more experienced traders.\n",
    "\n",
    "But the fundamental question remains: Is there truly something within a price chart that can provide reliable insight into the future movement of a stock? In other words, is the market, to some extent, predictable?\n",
    "\n",
    "If human analysts can identify price movement patterns through extensive data analysis, then it suggests a potential correlation between specific features and price fluctuations. Consequently, if such a correlation exists, machine learning models should also be capable of capturing it effectively.\n",
    "\n",
    "This notebook aims to explore the feasibility of achieving a meaningful level of accuracy in stock price prediction from scratch. The process involves autonomously collecting data, constructing a dataset enriched with relevant indicators, and ultimately training a machine learning model to forecast future prices.\n",
    "\n",
    "Regarding predictive precision, expecting to forecast exact prices would be overly ambitious and is not the primary objective of this study. Instead, a binary classification approach is adopted: the model predicts whether the next candlestick’s closing price will be higher (1) or lower (0) than the current one. Based on these signals, a trading strategy could involve taking a long position on a \"1\" signal and a short position on a \"0\" signal.\n",
    "\n",
    "The notebook is structured into four main sections:\n",
    "\n",
    "- **Data Gathering and Processing**\n",
    "- **ML Model Creation**\n",
    "- **Results Analysis**\n",
    "- **Future Improvements and References**\n",
    "\n",
    "Throughout this study, the following technical terms will be used:\n",
    "\n",
    "- **Candle**: object represents the price range of an asset over a specific period, indicating the open, close, high, and low prices, commonly used in market analysis.\n",
    "- **Technical Indicator**: A mathematical calculation based on historical price, volume, or open interest data, utilized in financial markets to analyze trends, patterns, and potential future price movements.\n",
    "- **Leverage**: The practice of using borrowed capital (via margin accounts) to amplify position sizes. Traders borrow funds from brokers to control larger trades than their own capital allows, increasing both potential gains and risks.\n",
    "- **Stop-Loss**: A risk management order placed with a broker to automatically sell a security when it reaches a predetermined price, limiting potential losses on a position.\n",
    "- **Take-Profit**: A predefined order to sell a security once it reaches a specific price level, ensuring that profits are realized before unfavorable market movements occur.\n",
    "\n",
    "This study aims to assess the potential of machine learning in stock price prediction while acknowledging the inherent complexities and uncertainties of financial markets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install yfinance==0.2.61\n",
    "!pip install pandas==2.2.2\n",
    "!pip install numpy==1.26.4\n",
    "!pip install matplotlib==3.9.2\n",
    "!pip install mlxtend==0.23.3\n",
    "!pip install imblearn==0.0\n",
    "!pip install scikit-learn==1.5.2\n",
    "!pip install missingno==0.5.2\n",
    "!pip install alpaca_trade_api==3.2.0\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "29c23a3bfb2c4510a0d06a9258fdc563",
    "deepnote_cell_type": "code",
    "execution_context_id": "b9f373fa-e949-465c-8e5e-6abf01d49c8f",
    "execution_millis": 98,
    "execution_start": 1735413867415,
    "source_hash": "b4c8874c"
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import warnings\n",
    "import alpaca_trade_api as tradeapi\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "fbe94cca8c1e42bc87f6edca1f693640",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": []
   },
   "source": [
    "# Part 1 -  Data gathering/processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6b0af585e87247e5bd317df340383671",
    "deepnote_cell_type": "text-cell-p",
    "formattedRanges": []
   },
   "source": [
    "For this project, a large dataset is required—one that is both up-to-date and reliable. While pre-existing datasets from platforms such as Kaggle can be useful, there is no guarantee that they are clean, well-maintained, or updated regularly. Additionally, to ensure flexibility in selecting different stocks for analysis and adjusting date ranges without constraints, it is essential to have full control over data collection and preprocessing.\n",
    "\n",
    "Thus, the approach involves downloading and cleaning the data independently.\n",
    "\n",
    "This section of the notebook is structured into four key components:\n",
    "\n",
    "- **1.1 Data Downloading**: Retrieving raw data from relevant sources.\n",
    "- **1.2 Feature Calculation Functions**: Computing key features necessary for model input.\n",
    "- **1.3 Label Generation**: Defining target variables for supervised learning.\n",
    "- **Main Data Preprocessing \"Pipeline\"**: A wrapper to streamline and manage the entire preprocessing workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "60b9fe033ceb42f1966d67a9f03d97b5",
    "deepnote_app_block_visible": false,
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": [],
    "is_collapsed": false
   },
   "source": [
    "## 1.1- Data downloading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, it is essential to download data from a reliable (and preferably free) source. Two potential sources meeting these criteria have been identified: Yahoo Finance and Alpaca API. Both are excellent choices; however, each comes with its own set of challenges. Yahoo Finance provides live data without delays and covers virtually every available financial instrument, but its access wrapper is unofficial, and intraday candle data is limited to 30 days. On the other hand, Alpaca API offers an official, authorized library with no restrictions on intraday candle data, though its data is not live and it does not encompass all stocks and financial instruments.\n",
    "\n",
    "Consequently, I have chosen to utilize both sources concurrently. Older data, which is not available via Yahoo Finance, is obtained from Alpaca API, while the most recent data is sourced from Yahoo Finance. Fortunately, the data formats from these two sources are fully compatible.\n",
    "\n",
    "For Alpaca API, a demo account was employed solely for data retrieval, ensuring that no real funds are involved. The entire data acquisition process was encapsulated in a function to facilitate easier access to all necessary parameters.\n",
    "\n",
    "Both data sources return a DataFrame with the following columns:\n",
    "- Datetime\n",
    "- Open\n",
    "- High\n",
    "- Low\n",
    "- Adj Close\n",
    "- Volume\n",
    "\n",
    "Each row represents a candle, and the timeframe for these candles can be selected from various durations. During the study and optimization phase, the following timeframes proved particularly interesting: 5 minutes, 10 minutes, 1 hour, and 1 day. Naturally, one would expect better performance with longer timeframes, as shorter 5- to 10-minute candles tend to contain more background noise, making it more challenging to discern correlations between features and price.\n",
    "\n",
    "Nevertheless, the 5-minute candles offer several advantages, such as the potential to execute a greater number of trades within a single day, thereby yielding more data points per trading day. This results in a database that is considerably less dated. For instance, while daily candles might accumulate only 8,000 to 10,000 instances over 31 to 39 years—accounting for market closures and holidays—5-minute candles may represent just 6 months of data. Consequently, the latter data is much more recent and coherent.\n",
    "\n",
    "However, it might be worth considering the use of the hourly candle as it provides data that is easy enough to obtain, but it also reduces the background noise present in the 5-minute candle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "3bb3a698653e4babb408e78849e6abf8",
    "deepnote_cell_type": "code",
    "execution_context_id": "ad971d17-8ec8-427f-934f-2fce021fc557",
    "execution_millis": 0,
    "execution_start": 1735413867564,
    "source_hash": "95b657c5"
   },
   "outputs": [],
   "source": [
    "def download_data_history(candle_time: int =15, path: str = r\"assets/data_history.csv\", download_option=\"yf\", ticker=\"^GSPC\", days_delta=100, currency=\"USD\") -> None:\n",
    "    try:\n",
    "        os.mkdir(\"assets\")\n",
    "    except: \n",
    "        pass\n",
    "    \n",
    "    # WARNING: API keys are exposed here only because they belong to a demo account used exclusively for data download\n",
    "    # This is NOT recommended practice. In production, always store sensitive credentials in .env files\n",
    "    API_KEY = 'PKJY9KJHWG0459RC02Y4'\n",
    "    API_SECRET = '6UXCVPutT2hpqQcUnQjFc74RSxcqrezcD4AXm0aL'\n",
    "    BASE_URL = 'https://paper-api.alpaca.markets'  \n",
    "    api = tradeapi.REST(API_KEY, API_SECRET, BASE_URL, api_version='v2')\n",
    "    print(download_option)\n",
    "    if download_option == \"yf\":\n",
    "        end_date = (datetime.now() + timedelta(days=1))\n",
    "        start_date = end_date - timedelta(days=days_delta)\n",
    "        tkr = yf.Ticker(ticker)\n",
    "        df = tkr.history(start=start_date.strftime('%Y-%m-%d'),\n",
    "                   end=end_date.strftime('%Y-%m-%d'),\n",
    "                   interval=candle_time)\n",
    "        df.reset_index(inplace=True)\n",
    "\n",
    "        if df['Volume'].iloc[0] == 0 and len(df) > 1:\n",
    "            df['Volume'].iloc[0] = df['Volume'].iloc[1:6].mean()\n",
    "        if \"d\" in candle_time:\n",
    "            if df['Date'].dt.tz is not None:\n",
    "                df['Date'] = df['Date'].dt.tz_convert(None)\n",
    "        df.rename(columns={'Date': 'Datetime', 'Close':'Adj Close'}, inplace=True)\n",
    "        df.drop([\"Dividends\",'Stock Splits'], axis=1, inplace=True)\n",
    "        try:\n",
    "            df.drop([\"Capital Gains\"], axis=1, inplace=True)\n",
    "        except:\n",
    "            pass\n",
    "        df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "        df = df[~((df['Datetime'].dt.date == pd.to_datetime('2024-06-30').date()) & (df['Volume'] == 0))]\n",
    "        df = df[~((df['Datetime'].dt.date == pd.to_datetime('2024-07-01').date()) & (df['Volume'] == 0))]\n",
    "        df.to_csv(path, index=False)\n",
    "    \n",
    "    elif download_option == \"alpaca\":\n",
    "        symbol = ticker  # Use provided ticker\n",
    "        timeframe = f\"{candle_time}\"  # Adjust for hourly candles\n",
    "        end_date = datetime.utcnow()\n",
    "        start_date = end_date - timedelta(days=days_delta)\n",
    "        all_data = []\n",
    "\n",
    "        while start_date < end_date:\n",
    "            chunk_end = start_date + timedelta(days=30)  # Adjust chunk size if needed\n",
    "            if chunk_end > end_date:\n",
    "                chunk_end = end_date\n",
    "\n",
    "            start_str = start_date.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "            end_str = chunk_end.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "            try:\n",
    "                data = api.get_bars(symbol, timeframe, start=start_str, end=end_str, feed=\"iex\").df\n",
    "                if not data.empty:\n",
    "                    all_data.append(data)\n",
    "                    print(len(data))\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching data from {start_str} to {end_str}: {e}\")\n",
    "\n",
    "            start_date = chunk_end\n",
    "\n",
    "        if all_data:\n",
    "            print(len(all_data))\n",
    "            historical_data = pd.concat(all_data)\n",
    "            historical_data['timestamp'] = historical_data.index.tz_localize(None)\n",
    "            historical_data.rename(columns={\"timestamp\": \"Datetime\", 'close': 'Adj Close', \"high\": \"High\", \"low\": \"Low\", \"volume\": \"Volume\", \"open\": \"Open\"}, inplace=True)\n",
    "            historical_data.dropna(inplace=True)\n",
    "            historical_data.to_csv(path, index=False)\n",
    "            print(\"Data saved successfully!\")\n",
    "        else:\n",
    "            print(\"No data retrieved.\")\n",
    "\n",
    "\n",
    "    elif download_option == \"mixed\":\n",
    "        #This options allows me to download both the data from yf and from alpacaapi,the two apis return similar df, ony small agiustments are needed.\n",
    "        end_date_yf= (datetime.now() + pd.Timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "        start_date_yf = (datetime.now() - pd.Timedelta(days=58)).strftime('%Y-%m-%d')\n",
    "        end_date_alpaca= (datetime.now() - pd.Timedelta(days=60)).strftime('%Y-%m-%d')\n",
    "        start_date_slpaca = (datetime.now() - pd.Timedelta(days=days_delta)).strftime('%Y-%m-%d')\n",
    "        tkr = yf.Ticker(ticker)\n",
    "        df_yf = tkr.history(start=start_date_yf, end=end_date_yf, interval=f\"{candle_time}h\")\n",
    "        df_yf.drop(\"Close\", axis=1, inplace=True)\n",
    "        df_yf.reset_index(inplace=True)\n",
    "        df_yf['Datetime'] = df_yf['Datetime'].dt.tz_localize(None)\n",
    "        df_alpaca = api.get_bars(symbol=ticker, timeframe=f\"{candle_time}Hour\", start=start_date_slpaca, end=end_date_alpaca).df\n",
    "        df_alpaca.reset_index(inplace=True)\n",
    "        df_alpaca.rename(columns={\"timestamp\":\"Datetime\",'close': 'Adj Close',\"high\":\"High\", \"low\":\"Low\",\"volume\":\"Volume\",\"open\":\"Open\"}, inplace=True)\n",
    "        df_alpaca['Datetime'] = df_alpaca['Datetime'].dt.tz_localize(None)  \n",
    "        df_alpaca.drop([\"trade_count\",\"vwap\"], axis=1, inplace=True)\n",
    "        df_alpaca = df_alpaca[[\"Datetime\", \"Open\", \"High\", \"Low\", \"Adj Close\", \"Volume\"]]\n",
    "\n",
    "        df_combined = pd.concat([df_alpaca, df_yf], axis=0)\n",
    "\n",
    "        df_combined.reset_index(drop=True, inplace=True)\n",
    "        df_combined.dropna(inplace=True)\n",
    "        df_combined.to_csv(path, index=False)\n",
    "        \n",
    "\n",
    "\n",
    "    elif download_option == \"crypto\":\n",
    "        symbol = ticker  # Use the provided ticker, e.g. \"BTCUSD\"\n",
    "        timeframe = f\"{candle_time}\"  # For example, \"1H\" for hourly candles\n",
    "        end_date = datetime.utcnow()\n",
    "        start_date = end_date - timedelta(days=days_delta)\n",
    "        all_data = []\n",
    "\n",
    "        while start_date < end_date:\n",
    "            chunk_end = start_date + timedelta(days=200)  # You can adjust chunk size if needed\n",
    "            if chunk_end > end_date:\n",
    "                chunk_end = end_date\n",
    "\n",
    "            start_str = start_date.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "            end_str = chunk_end.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "            try:\n",
    "                # Fetch crypto data (ensure to set the correct exchange parameter)\n",
    "                data = api.get_crypto_bars(symbol, timeframe, start=start_str, end=end_str).df\n",
    "                if not data.empty:\n",
    "                    all_data.append(data)\n",
    "                    print(f\"Fetched {len(data)} rows from {start_str} to {end_str}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching data from {start_str} to {end_str}: {e}\")\n",
    "\n",
    "            start_date = chunk_end\n",
    "\n",
    "        if all_data:\n",
    "            print(f\"Number of chunks retrieved: {len(all_data)}\")\n",
    "            historical_data = pd.concat(all_data)\n",
    "            historical_data['timestamp'] = historical_data.index.tz_localize(None)\n",
    "            historical_data.rename(columns={\n",
    "                \"timestamp\": \"Datetime\",\n",
    "                \"close\": \"Adj Close\",\n",
    "                \"high\": \"High\",\n",
    "                \"low\": \"Low\",\n",
    "                \"volume\": \"Volume\",\n",
    "                \"open\": \"Open\"\n",
    "            }, inplace=True)\n",
    "            historical_data.drop([\"trade_count\",\"vwap\",\"symbol\"], axis=1, inplace=True)\n",
    "            historical_data.dropna(inplace=True)\n",
    "            historical_data.to_csv(path, index=False)\n",
    "            print(\"Data saved successfully!\")\n",
    "        else:\n",
    "            print(\"No data retrieved.\")\n",
    "\n",
    "\n",
    "    elif download_option == \"mt5\":\n",
    "        try:\n",
    "            import MetaTrader5 as mt5\n",
    "            \n",
    "            # Initialize MT5 connection\n",
    "            if not mt5.initialize():\n",
    "                print(f\"MT5 initialization failed. Error code: {mt5.last_error()}\")\n",
    "                return\n",
    "            \n",
    "            # Get the symbol from user input\n",
    "            symbol = ticker\n",
    "            \n",
    "            # Get timeframe from user input\n",
    "            timeframe_options = {\n",
    "                \"1m\": mt5.TIMEFRAME_M1,\n",
    "                \"5m\": mt5.TIMEFRAME_M5,\n",
    "                \"10m\": mt5.TIMEFRAME_M10,\n",
    "                \"15m\": mt5.TIMEFRAME_M15,\n",
    "                \"30m\": mt5.TIMEFRAME_M30,\n",
    "                \"1h\": mt5.TIMEFRAME_H1,\n",
    "                \"4h\": mt5.TIMEFRAME_H4,\n",
    "                \"1d\": mt5.TIMEFRAME_D1,\n",
    "                \"1w\": mt5.TIMEFRAME_W1,\n",
    "                \"1mn\": mt5.TIMEFRAME_MN1\n",
    "            }\n",
    "            \n",
    "            \n",
    "            timeframe_input = candle_time\n",
    "            if timeframe_input not in timeframe_options:\n",
    "                print(f\"Invalid timeframe. Using default 1d.\")\n",
    "                timeframe = mt5.TIMEFRAME_D1\n",
    "            else:\n",
    "                timeframe = timeframe_options[timeframe_input]\n",
    "            \n",
    "            # Get date range using days_delta\n",
    "            try:\n",
    "                days_delta = int(days_delta)\n",
    "            except ValueError:\n",
    "                print(\"Invalid number of days. Using default 365 days.\")\n",
    "                days_delta = 365\n",
    "            \n",
    "            to_date = datetime.now()\n",
    "            from_date = to_date - timedelta(days=days_delta)\n",
    "            \n",
    "            # Request historical data\n",
    "            rates = mt5.copy_rates_range(symbol, timeframe, from_date, to_date)\n",
    "            \n",
    "            if rates is None or len(rates) == 0:\n",
    "                print(f\"No data retrieved for {symbol}. Error: {mt5.last_error()}\")\n",
    "                mt5.shutdown()\n",
    "                return\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(rates)\n",
    "            \n",
    "            # Convert time in seconds into datetime format\n",
    "            df['Datetime'] = pd.to_datetime(df['time'], unit='s')\n",
    "            \n",
    "            # Drop the original time column\n",
    "            df = df.drop(columns=['time', 'spread', 'real_volume'])\n",
    "            \n",
    "            # Rename columns to match our standard format\n",
    "            df.rename(columns={\n",
    "                'open': 'Open',\n",
    "                'high': 'High',\n",
    "                'low': 'Low',\n",
    "                'close': 'Adj Close',\n",
    "                'tick_volume': 'Volume'\n",
    "            }, inplace=True)\n",
    "            \n",
    "            # Reorder columns\n",
    "            df = df[['Datetime', 'Open', 'High', 'Low', 'Adj Close', 'Volume']]\n",
    "            # Save to CSV\n",
    "            df.to_csv(path, index=False)\n",
    "            \n",
    "            print(f\"Downloaded {len(df)} records for {symbol} from {from_date.strftime('%Y-%m-%d')} to {to_date.strftime('%Y-%m-%d')}\")\n",
    "            # Shutdown MT5 connection\n",
    "            mt5.shutdown()\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"MetaTrader5 module not found. Please install it using: pip install MetaTrader5\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading data from MT5: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "eacfee8c1f3d46058798246827b4e822",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## 1.2- Feature Calculation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, various features will be added for each candlestick, consisting of multiple financial technical indicators. A broader set of indicators is initially included, extending beyond those deemed immediately useful. The goal is to later refine the selection through feature reduction techniques, ensuring that only the most relevant indicators contribute to the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pct_values(df):\n",
    "    df[\"pct_close\"] = (1-df[\"Adj Close\"]/df[\"Adj Close\"].shift(1))*100\n",
    "    df[\"pct_open\"] = (1-df[\"Open\"]/df[\"Open\"].shift(1))*100\n",
    "    df[\"ptc_high\"] = (1-df[\"High\"]/df[\"Open\"])*100\n",
    "    df[\"ptc_low\"] = (1-df[\"Low\"]/df[\"Open\"])*100\n",
    "    df.drop([\"Adj Close\",'Open',\"High\",\"Low\"], axis=1, inplace=True)\n",
    "    df.rename(columns={\"pct_close\": \"Adj Close\", 'pct_open': 'Open', \"ptc_high\": \"High\", \"ptc_low\": \"Low\"}, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_std(price_column):\n",
    "    rolling_std = price_column.rolling(window=14).std()\n",
    "    return rolling_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_spread_average(high_column, low_column, window=14):\n",
    "    spread = high_column - low_column    \n",
    "    spread_average = spread.rolling(window=window).mean()    \n",
    "    return spread_average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_volume_average(volume_column, window=14):    \n",
    "    volume_average = volume_column.rolling(window=window).mean()    \n",
    "    return volume_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_range(high_column, low_column):\n",
    "    # Compute rolling highest high and lowest low\n",
    "    max_high = high_column.rolling(window=14).max()\n",
    "    min_low = low_column.rolling(window=14).min()\n",
    "    # Compute range\n",
    "    rolling_range = max_high - min_low\n",
    "    return rolling_range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_price_rate_of_change(price_column):\n",
    "    # Calculate the previous price by shifting the price column by one period *backward*\n",
    "    previous_price = price_column.shift(1)  # This gives you the price from the previous period\n",
    "    # Calculate the Rate of Change\n",
    "    roc = (price_column - previous_price) / previous_price\n",
    "    return roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_log_returns(price_column):\n",
    "    previous_prices = price_column.shift(1)  # Get the previous period's prices\n",
    "    log_returns = np.log(price_column / previous_prices)\n",
    "    return log_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_volume_delta(volume_column):\n",
    "    \"\"\"\n",
    "    Calculate the Volume Delta between the current and previous volume.\n",
    "\n",
    "    Args:\n",
    "        volume_column (pandas Series): Trading volumes from most recent to oldest.\n",
    "    Returns:\n",
    "        pandas Series: Current volume minus previous volume\n",
    "    \"\"\"\n",
    "    previous_volume = volume_column.shift(1)\n",
    "    volume_delta = volume_column - previous_volume\n",
    "    return volume_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_price_to_volume_ratio(close_column, volume_column):\n",
    "    \"\"\"\n",
    "    Calculate the Price to Volume Ratio.\n",
    "\n",
    "    :param close_column: pandas Series of closing prices\n",
    "    :param volume_column: pandas Series of trading volumes\n",
    "    :return: pandas Series of Price to Volume Ratios\n",
    "    \"\"\"\n",
    "    volume_column = volume_column.replace(0, np.nan)\n",
    "    price_to_volume_ratio = close_column / volume_column\n",
    "    return price_to_volume_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cumulative_return(price_column):\n",
    "    \"\"\"\n",
    "    Calculate the cumulative return over a rolling window of 14 periods.\n",
    "\n",
    "    Args:\n",
    "        price_column (pandas Series): A pandas Series of prices in chronological order (oldest to most recent).\n",
    "        \n",
    "    Returns:\n",
    "        pandas Series: The cumulative return for each row.\n",
    "    \"\"\"\n",
    "    def rolling_cumulative_return(prices):\n",
    "        returns = prices.pct_change().dropna()\n",
    "        if len(returns) < 13:  # Because 14 prices -> 13 returns\n",
    "            return None\n",
    "        cumulative_return = (1 + returns).prod() - 1\n",
    "        return cumulative_return\n",
    "\n",
    "    rolling_returns = price_column.rolling(window=14, min_periods=14).apply(rolling_cumulative_return, raw=False)\n",
    "    return rolling_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ema(adj_close_column, span=14):\n",
    "    \"\"\"\n",
    "    Calculate the Exponential Moving Average (EMA) for a series.\n",
    "\n",
    "    Args:\n",
    "        adj_close_column (pandas Series): Adjusted closing prices in chronological order (oldest to most recent).\n",
    "        span (int): EMA period.\n",
    "\n",
    "    Returns:\n",
    "        pandas Series: EMA values.\n",
    "    \"\"\"\n",
    "    ema_series = adj_close_column.ewm(span=span, adjust=False).mean()\n",
    "    ema_series.iloc[:span] = None  # Set the first `span` values to NaN due to insufficient data\n",
    "    return ema_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rsi(adj_close_column, period=14):\n",
    "    \"\"\"\n",
    "    Calculate the Relative Strength Index (RSI) for a pandas Series of adjusted closing prices.\n",
    "    \n",
    "    Args:\n",
    "        adj_close_column (Series): Adjusted closing prices in chronological order (oldest to most recent).\n",
    "        period (int): Period over which to calculate RSI. Default is 14.\n",
    "\n",
    "    Returns:\n",
    "        Series: RSI values between 0 and 100.\n",
    "    \"\"\"\n",
    "    # Calculate price changes (deltas) between consecutive periods\n",
    "    deltas = adj_close_column.diff()\n",
    "\n",
    "    # Separate gains and losses\n",
    "    gains = deltas.where(deltas > 0, 0)\n",
    "    losses = -deltas.where(deltas < 0, 0)\n",
    "\n",
    "    # Calculate average gain and loss\n",
    "    avg_gain = gains.rolling(window=period).mean()\n",
    "    avg_loss = losses.rolling(window=period).mean()\n",
    "\n",
    "    # Smooth the averages\n",
    "    avg_gain = avg_gain.ewm(span=period, adjust=False).mean()\n",
    "    avg_loss = avg_loss.ewm(span=period, adjust=False).mean()\n",
    "\n",
    "    # Avoid division by zero\n",
    "    avg_loss = avg_loss.replace(0, 1e-10)\n",
    "\n",
    "    # Calculate RSI\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "\n",
    "    return rsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_macd(df):\n",
    "    \"\"\"\n",
    "    Calculate the Moving Average Convergence Divergence (MACD) indicator.\n",
    "\n",
    "    Steps:\n",
    "    1. Calculate 12-period EMA of adjusted closing prices\n",
    "    2. Calculate 26-period EMA of adjusted closing prices  \n",
    "    3. MACD Line = 12-period EMA - 26-period EMA\n",
    "    4. Signal Line = 9-period EMA of the MACD Line\n",
    "    5. MACD Histogram = MACD Line - Signal Line\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame containing the 'Adj Close' prices in chronological order (oldest to most recent).\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The same DataFrame with added MACD-related columns.\n",
    "    \"\"\"\n",
    "    df['EMA_12'] = df['Adj Close'].ewm(span=12, adjust=False).mean()\n",
    "    df['EMA_26'] = df['Adj Close'].ewm(span=26, adjust=False).mean()\n",
    "    df['MACD'] = df['EMA_12'] - df['EMA_26']\n",
    "    df['Signal_Line'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
    "    df['MACD_Histogram'] = df['MACD'] - df['Signal_Line']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean(close_column):\n",
    "    return close_column.rolling(window=14).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bollinger_upper_bands(price_column, num_std_dev=2):\n",
    "    \"\"\"\n",
    "    Calculate the upper Bollinger Band.\n",
    "\n",
    "    Parameters:\n",
    "    price_column (pandas Series): Prices in chronological order (oldest to most recent).\n",
    "    num_std_dev (float): Number of standard deviations to add to the mean (default is 2).\n",
    "\n",
    "    Returns:\n",
    "    pandas Series: Upper Bollinger Band.\n",
    "    \"\"\"\n",
    "    mean = price_column.rolling(window=14).mean()\n",
    "    std_dev = price_column.rolling(window=14).std()\n",
    "    upper_band = mean + (num_std_dev * std_dev)\n",
    "    return upper_band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bollinger_lower_bands(price_column, num_std_dev=2):\n",
    "    \"\"\"\n",
    "    Calculate the lower Bollinger Band.\n",
    "\n",
    "    Parameters:\n",
    "    price_column (pandas Series): Prices in chronological order (oldest to most recent).\n",
    "    num_std_dev (float): Number of standard deviations to subtract from the mean (default is 2).\n",
    "\n",
    "    Returns:\n",
    "    pandas Series: Lower Bollinger Band.\n",
    "    \"\"\"\n",
    "    mean = price_column.rolling(window=14).mean()\n",
    "    std_dev = price_column.rolling(window=14).std()\n",
    "    lower_band = mean - (num_std_dev * std_dev)\n",
    "    return lower_band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_atr(high_column, low_column, close_column, period=14):\n",
    "    \"\"\"\n",
    "    Calculate the Average True Range (ATR) using Wilder's smoothing method.\n",
    "\n",
    "    Parameters:\n",
    "        high_column (pd.Series): High prices in chronological order.\n",
    "        low_column (pd.Series): Low prices in chronological order.\n",
    "        close_column (pd.Series): Close prices in chronological order.\n",
    "        period (int): Period for ATR calculation (default is 14).\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: ATR values.\n",
    "    \"\"\"\n",
    "    # True Range components\n",
    "    tr1 = high_column - low_column\n",
    "    tr2 = (high_column - close_column.shift(1)).abs()\n",
    "    tr3 = (low_column - close_column.shift(1)).abs()\n",
    "    \n",
    "    # Combine into True Range\n",
    "    true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "\n",
    "    # Initialize ATR Series with NaNs\n",
    "    atr = pd.Series(index=true_range.index, dtype='float64')\n",
    "    \n",
    "    # First ATR is simple average\n",
    "    atr.iloc[period - 1] = true_range.iloc[:period].mean()\n",
    "    \n",
    "    # Wilder's smoothing for the rest\n",
    "    for i in range(period, len(true_range)):\n",
    "        atr.iloc[i] = (atr.iloc[i - 1] * (period - 1) + true_range.iloc[i]) / period\n",
    "\n",
    "    return atr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_plus_di(high, low, close, window=14):\n",
    "    \"\"\"\n",
    "    Calculate +DI (Positive Directional Indicator).\n",
    "\n",
    "    Parameters:\n",
    "        high (pd.Series): High prices in chronological order.\n",
    "        low (pd.Series): Low prices in chronological order.\n",
    "        close (pd.Series): Close prices in chronological order.\n",
    "        window (int): Rolling window size (default: 14).\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: +DI values.\n",
    "    \"\"\"\n",
    "    up_move = high.diff()\n",
    "    down_move = low.diff().abs()\n",
    "\n",
    "    plus_dm = up_move.where((up_move > down_move) & (up_move > 0), 0)\n",
    "\n",
    "    tr = pd.concat([\n",
    "        (high - low),\n",
    "        (high - close.shift(1)).abs(),\n",
    "        (low - close.shift(1)).abs()\n",
    "    ], axis=1).max(axis=1)\n",
    "\n",
    "    smoothed_plus_dm = plus_dm.rolling(window=window).mean()\n",
    "    atr = tr.rolling(window=window).mean()\n",
    "\n",
    "    plus_di = 100 * (smoothed_plus_dm / atr)\n",
    "    return plus_di\n",
    "\n",
    "def calculate_minus_di(high, low, close, window=14):\n",
    "    \"\"\"\n",
    "    Calculate -DI (Negative Directional Indicator).\n",
    "\n",
    "    Parameters:\n",
    "        high (pd.Series): High prices in chronological order.\n",
    "        low (pd.Series): Low prices in chronological order.\n",
    "        close (pd.Series): Close prices in chronological order.\n",
    "        window (int): Rolling window size (default: 14).\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: -DI values.\n",
    "    \"\"\"\n",
    "    up_move = high.diff().abs()\n",
    "    down_move = low.diff()\n",
    "\n",
    "    minus_dm = down_move.where((down_move > up_move) & (down_move > 0), 0)\n",
    "\n",
    "    tr = pd.concat([\n",
    "        (high - low),\n",
    "        (high - close.shift(1)).abs(),\n",
    "        (low - close.shift(1)).abs()\n",
    "    ], axis=1).max(axis=1)\n",
    "\n",
    "    smoothed_minus_dm = minus_dm.rolling(window=window).mean()\n",
    "    atr = tr.rolling(window=window).mean()\n",
    "\n",
    "    minus_di = 100 * (smoothed_minus_dm / atr)\n",
    "    return minus_di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_adx(high, low, close, window=14):\n",
    "    \"\"\"\n",
    "    Calculate the Average Directional Index (ADX) using high, low, and close prices.\n",
    "\n",
    "    Parameters:\n",
    "        high (pd.Series): High prices in chronological order.\n",
    "        low (pd.Series): Low prices in chronological order.\n",
    "        close (pd.Series): Close prices in chronological order.\n",
    "        window (int): Rolling window size (default: 14).\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: ADX values.\n",
    "    \"\"\"\n",
    "    up_move = high.diff()\n",
    "    down_move = low.diff()\n",
    "\n",
    "    plus_dm = up_move.where((up_move > down_move) & (up_move > 0), 0)\n",
    "    minus_dm = (-down_move).where((down_move < up_move) & (down_move < 0), 0)\n",
    "\n",
    "    tr = pd.concat([\n",
    "        high - low,\n",
    "        (high - close.shift(1)).abs(),\n",
    "        (low - close.shift(1)).abs()\n",
    "    ], axis=1).max(axis=1)\n",
    "\n",
    "    atr = tr.rolling(window=window).mean()\n",
    "    smoothed_plus_dm = plus_dm.rolling(window=window).mean()\n",
    "    smoothed_minus_dm = minus_dm.rolling(window=window).mean()\n",
    "\n",
    "    plus_di = 100 * (smoothed_plus_dm / atr)\n",
    "    minus_di = 100 * (smoothed_minus_dm / atr)\n",
    "\n",
    "    dx = (abs(plus_di - minus_di) / (plus_di + minus_di).replace(0, np.nan)) * 100\n",
    "    adx = dx.rolling(window=window).mean()\n",
    "\n",
    "    return adx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_time_of_day_sin(datetime_col):\n",
    "    \"\"\"\n",
    "    Calculate sine component of time of day encoding.\n",
    "    \n",
    "    Parameters:\n",
    "    datetime_col (pandas.Series): Datetime values\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Sine values for time of day (0 to 2π), both 0:00 and 24:00 map to the same point\"\"\"\n",
    "    minutes_in_day = datetime_col.dt.hour * 60 + datetime_col.dt.minute\n",
    "    return np.sin(2 * np.pi * minutes_in_day / 1440)\n",
    "\n",
    "def calculate_time_of_day_cos(datetime_col):\n",
    "    \"\"\"\n",
    "    Calculate the cosine component of cyclical time of day encoding.\n",
    "    \n",
    "    Parameters:\n",
    "    datetime_col (pandas.Series): Column containing datetime values\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Cosine values representing time of day from 0 to 2π,\n",
    "                  where both 0:00 and 24:00 map to the same point\n",
    "    \"\"\"\n",
    "    minutes_in_day = datetime_col.dt.hour * 60 + datetime_col.dt.minute\n",
    "    return np.cos(2 * np.pi * minutes_in_day / 1440)\n",
    "\n",
    "\n",
    "def calculate_day_of_week_sin(datetime_col):\n",
    "    \"\"\"\n",
    "    Calculate the sine component of cyclical day of week encoding.\n",
    "    \n",
    "    Parameters:\n",
    "    datetime_col (pandas.Series): Column containing datetime values\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Sine values representing days from 0 to 2π,\n",
    "                  where both Sunday and Saturday map to the same point\n",
    "    \"\"\"\n",
    "    return np.sin(2 * np.pi * datetime_col.dt.dayofweek / 7)\n",
    "\n",
    "\n",
    "def calculate_day_of_week_cos(datetime_col):\n",
    "    \"\"\"\n",
    "    Calculate the cosine component of cyclical day of week encoding.\n",
    "    \n",
    "    Parameters:\n",
    "    datetime_col (pandas.Series): Column containing datetime values\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Cosine values representing days from 0 to 2π,\n",
    "                  where both Sunday and Saturday map to the same point\n",
    "    \"\"\"\n",
    "    return np.cos(2 * np.pi * datetime_col.dt.dayofweek / 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5ab233ceedf948c7a8503323f5eaed8d",
    "deepnote_cell_type": "text-cell-h2",
    "formattedRanges": []
   },
   "source": [
    "## 1.3- Feature Calculation Functions Wrapper  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I implemented a wrapper function to integrate all the features. The process begins with the base features for each instance:\n",
    "\n",
    "- **Open**\n",
    "- **High**\n",
    "- **Low**\n",
    "- **Adj Close**\n",
    "- **Volume**\n",
    "\n",
    "Subsequently, I calculate additional features derived from these base values:\n",
    "\n",
    "- **Price Rate of Change**\n",
    "- **Log Returns**\n",
    "- **Volume Delta**\n",
    "- **Price-Volume Ratio**\n",
    "\n",
    "After computing these, I replicate all of the features by incorporating data from the previous *n* instances. For each feature (e.g., Open), the values from the preceding instances are appended as separate features (e.g., Open-1, Open-2, Open-3, ..., Open-n), and this replication is applied to all the calculated features.\n",
    "\n",
    "The previously mentioned set of features, referred to as \"repeated features,\" constitutes the first group. In contrast, there exists another group of features which, once computed for an individual instance, are not carried over to subsequent instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "06d70702b5d2425c98b4fd3e580ebae0",
    "deepnote_cell_type": "code",
    "execution_context_id": "ad971d17-8ec8-427f-934f-2fce021fc557",
    "execution_millis": 0,
    "execution_start": 1735413867687,
    "source_hash": "9784167d"
   },
   "outputs": [],
   "source": [
    "def extract_advanced_features(df_path, n_precedent_candles=14):\n",
    "\n",
    "    df = pd.read_csv(df_path, on_bad_lines=\"warn\")\n",
    "    for column in df.columns:\n",
    "        if column == \"Datetime\":\n",
    "            continue\n",
    "        df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    #repeated features\n",
    "    df['price_rate_of_change'] = calculate_price_rate_of_change(df['Adj Close'])\n",
    "    df['log_returns'] = calculate_log_returns(df['Adj Close'])\n",
    "    df[\"volume_delta\"] = calculate_volume_delta(df['Volume'])\n",
    "    df[\"price_volume_ratio\"] = calculate_price_to_volume_ratio(df['Adj Close'], df['Volume'])\n",
    "    try:\n",
    "        df.drop(\"Close\", axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    columns_names_list = [col for col in df.columns.values.tolist() if col != 'Datetime']\n",
    "    new_columns = {}\n",
    "    for n in range(1, n_precedent_candles + 1):\n",
    "        for i in columns_names_list:\n",
    "            new_columns[f\"{i}_n_{n}\"] = df[i].shift(+n)\n",
    "    df = pd.concat([df, pd.DataFrame(new_columns)], axis=1)\n",
    "\n",
    "    #Single time features\n",
    "    #df = candles_analizer(df)\n",
    "    df[\"rolling_std\"] = calculate_rolling_std(df[\"Adj Close\"])\n",
    "    df[\"rolling_spread_average\"] = calculate_rolling_spread_average(df[\"High\"], df[\"Low\"])\n",
    "    df[\"rolling_volume_average\"] = calculate_rolling_volume_average(df[\"Volume\"]) \n",
    "    df[\"rolling_range\"] = calculate_rolling_range(df[\"High\"], df[\"Low\"])\n",
    "    df[\"cumulative_return\"] = calculate_cumulative_return(df[\"Adj Close\"])\n",
    "    df[\"ema\"] = calculate_ema(df[\"Adj Close\"])\n",
    "    df[\"rsi\"] = calculate_rsi(df[\"Adj Close\"])\n",
    "    df[\"mean\"] = calculate_mean(df[\"Adj Close\"])\n",
    "    df = calculate_macd(df)  \n",
    "    df[\"upper_bollinger_bands\"] = calculate_bollinger_upper_bands(df[\"mean\"], df[\"rolling_std\"])\n",
    "    df[\"lower_bollinger_bands\"] = calculate_bollinger_lower_bands(df[\"mean\"], df[\"rolling_std\"])\n",
    "    df[\"average_true_range\"] = calculate_atr(df[\"High\"], df[\"Low\"], df[\"Adj Close\"])\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "    df['time_sin'] = calculate_time_of_day_sin(df['Datetime'])\n",
    "    df['time_cos'] = calculate_time_of_day_cos(df['Datetime'])\n",
    "    df['day_sin'] = calculate_day_of_week_sin(df['Datetime'])\n",
    "    df['day_cos'] = calculate_day_of_week_cos(df['Datetime'])\n",
    "    df[\"plus_di\"] = calculate_plus_di(df[\"High\"], df[\"Low\"], df[\"Adj Close\"])\n",
    "    df[\"minus_di\"] = calculate_minus_di(df[\"High\"], df[\"Low\"], df[\"Adj Close\"])\n",
    "    df[\"average_directional_index\"] = calculate_adx(df[\"High\"], df[\"Low\"], df[\"Adj Close\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "638bd13b1add4decacd54110501242f9",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## 1.4- Label Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This brief section is dedicated solely to highlighting the classification function. It has been maintained as a separate section to ensure a clear and organized code structure. The label is defined as follows:\n",
    "\n",
    "- **1**: Indicates that the adjusted closing price of the next candlestick will be greater than the current adjusted closing price (just concluded).\n",
    "- **0**: Indicates that the adjusted closing price of the next candlestick will be lower than the current adjusted closing price (just concluded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "35e97145579942e48ec8751415cccd52",
    "deepnote_cell_type": "code",
    "execution_context_id": "ad971d17-8ec8-427f-934f-2fce021fc557",
    "execution_millis": 0,
    "execution_start": 1735413867735,
    "source_hash": "fc267b11"
   },
   "outputs": [],
   "source": [
    "def add_classification_label(df):\n",
    "    \"\"\"\n",
    "    Add binary classification label based on price movement\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    \"\"\"\n",
    "    df[\"label\"] = (df[\"Adj Close\"].shift(-1) > df[\"Adj Close\"]).astype(int)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c1ca0425241248fc9443e882052a6218",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## 0.5- Data Preprocessing and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "d2988300e1544e79949e9568aee31550",
    "deepnote_cell_type": "code",
    "execution_context_id": "732eef6f-228b-49ac-b42d-9545e68c4eff",
    "execution_millis": 9058,
    "execution_start": 1735413867831,
    "source_hash": "7e73ac0d"
   },
   "outputs": [],
   "source": [
    "input_file_path = 'assets/data_history.csv'\n",
    "download_data_history(candle_time=\"1d\", path=input_file_path, download_option=\"yf\", ticker=\"^FTSE\", days_delta=10000)\n",
    "\n",
    "output_file_path_svm = 'assets/data_preprocessed_svm.csv'\n",
    "preprocessed_df = extract_advanced_features(input_file_path, n_precedent_candles=13)\n",
    "preprocessed_df = add_classification_label(preprocessed_df)\n",
    "preprocessed_df.dropna(inplace=True)\n",
    "\n",
    "preprocessed_df.set_index('Datetime', inplace=True)\n",
    "\n",
    "preprocessed_df.reset_index(inplace=True)\n",
    "\n",
    "preprocessed_df.to_csv(output_file_path_svm, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure the correctness of the function implementation and application, I visualized the distribution of all features through graphical representations. This approach allowed me to identify potential errors more effectively. It proved particularly useful during the model testing phase, as it enabled me to detect and correct mistakes in the application of certain feature transformations.\n",
    "\n",
    "This will also be useful later for choosing what type of feature scaling to apply "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [column for column in preprocessed_df.columns if \"_n_\" not in column]\n",
    "num_columns = len(cols)\n",
    "num_rows = (num_columns + 1) // 2  \n",
    "num_cols = 2  \n",
    "\n",
    "plt.figure(figsize=(15, 5 * num_rows))\n",
    "\n",
    "for i, column in enumerate(cols, 1):  \n",
    "    plt.subplot(num_rows, num_cols, i)\n",
    "    sns.histplot(preprocessed_df[column], kde=True)\n",
    "    plt.title(f'Distribution of {column}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "f5c97d9dbf2e4662bcc070ad875b8694",
    "deepnote_cell_type": "text-cell-h1",
    "formattedRanges": []
   },
   "source": [
    "# Part 2- ML model creation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the notebook, the objective is to identify an optimal machine learning model. This model will be structured as a pipeline comprising three key components:\n",
    " \n",
    "1. **Preprocessing Step**: This stage involves data transformation and feature engineering to enhance model performance.\n",
    "2. **Dimensionality Reduction (if necessary)**: If required, this step aims to reduce the number of input features while preserving essential information.\n",
    "3. **Classification Model**: A classifier will be selected and fine-tuned to achieve optimal predictive performance.\n",
    " \n",
    "The pipeline ensures a systematic approach to model development, improving efficiency and scalability while maintaining reproducibility.\n",
    " \n",
    "This section of the notebook is structured into 2 key components:\n",
    " \n",
    "- **2.1 Data loading, splitting and processing**: In this component, the preprocessed dataset is loaded, cleaned, and split into training and testing sets.\n",
    "- **2.2 Grid search**: In this component, an exhaustive grid search is employed alongside cross-validation to evaluate various hyperparameter combinations for the different stages of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Data loading, splitting and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "d5228b151b924467afb308efcb0fc463",
    "deepnote_cell_type": "code",
    "execution_context_id": "732eef6f-228b-49ac-b42d-9545e68c4eff",
    "execution_millis": 961,
    "execution_start": 1735413876939,
    "source_hash": "8d43343f"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import (train_test_split,\n",
    "                                     learning_curve, validation_curve, RandomizedSearchCV, \n",
    "                                     cross_validate, RepeatedStratifiedKFold)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import  MinMaxScaler\n",
    "from sklearn.preprocessing import  StandardScaler\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from imblearn.pipeline import Pipeline as IMBPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "import joblib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "c47e00b3870041af82f0a48cd8238fc6",
    "deepnote_cell_type": "code",
    "execution_context_id": "732eef6f-228b-49ac-b42d-9545e68c4eff",
    "execution_millis": 648,
    "execution_start": 1735413877947,
    "source_hash": "c999f1c1"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('assets/data_preprocessed_svm.csv')\n",
    "df.drop(index=df.index[0], axis=0, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "82fbc6ad99af49bbb59d15cabb500a0e",
    "deepnote_cell_type": "code",
    "execution_context_id": "5ea92291-0be3-4796-83b7-e3423b5b0cad",
    "execution_millis": 50,
    "execution_start": 1735413878751,
    "source_hash": "4132a54b"
   },
   "outputs": [],
   "source": [
    "X = df.drop(columns=['label'])\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, shuffle=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My dataset is already free of null values, and all features are numerical. Therefore, the only transformation required is data scaling to ensure consistency across different magnitudes.\n",
    "\n",
    "The only exception is the datetime column, which I will retain as an index. This will facilitate time-series visualization and allow for more intuitive plotting of trends in later stages of the analysis.\n",
    "\n",
    "To apply feature scaling, I used the previously plotted graphs to understand when to apply normalization (the data has a non-Gaussian distribution) and when to apply standardization (the data follows a normal distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline_complete = Pipeline([('scaler', MinMaxScaler(feature_range=(0,1)))])\n",
    "num_features = [col for col in df.select_dtypes(include=['float64', 'int64']).columns if col != 'label']\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('norm', pipeline_complete, num_features),])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline = IMBPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('dim_reduction', PCA(n_components=0.9)),  \n",
    "    ('smote', SMOTE(sampling_strategy='auto')),  \n",
    "    ('classifier', Perceptron())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, the objective was to maximize the **F1 score**, defined as:\n",
    "\n",
    "$$\n",
    "F1 = \\frac{2 \\times (\\text{Precision} \\times \\text{Recall})}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "However, optimizing for the F1 score caused the model to predominantly predict class 1, resulting in an unbalanced and ineffective classification. This happened probably because the F1 score focuses mainly on the positive class and does not penalize poor performance on the negative class.\n",
    "\n",
    "To mitigate this issue, a **custom scoring function** was implemented:\n",
    "\n",
    "$$\n",
    "\\text{Custom Precision Score} =\n",
    "\\begin{cases}\n",
    "0, & \\text{if } P_0 < 0.5 \\text{ or } P_1 < 0.5 \\text{ or } P_0 = 0 \\text{ or } P_1 = 0, \\\\\n",
    "\\left(P_0 \\times P_1 + \\max(P_0, P_1)\\right) \\times \\left(1 - \\lvert P_0 - P_1 \\rvert\\right), & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "While this custom metric led to some improvements, it still did not fully resolve the imbalance.\n",
    "\n",
    "Recognizing the need for a more robust evaluation metric, the **Matthews Correlation Coefficient (MCC)** was explored. The MCC is given by:\n",
    "\n",
    "$$\n",
    "\\text{MCC} = \\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}\n",
    "$$\n",
    "\n",
    "Because MCC accounts for all four elements of the confusion matrix (true positives, true negatives, false positives, and false negatives), it provides a more balanced and comprehensive assessment of the model's performance. Optimizing the model using MCC prevented the bias toward predicting class 1 and encouraged more balanced predictions across both classes.\n",
    "\n",
    "**Conclusion:**  \n",
    "Using the MCC as the primary evaluation metric leads to a more balanced classifier, effectively addressing the pitfalls of optimizing solely for the F1 score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, make_scorer\n",
    "def custom_precision_score(y_true, y_pred):\n",
    "    precision_0 = precision_score(y_true, y_pred, pos_label=0)\n",
    "    precision_1 = precision_score(y_true, y_pred, pos_label=1)    \n",
    "    if (precision_0 < 0.5 or precision_1 < 0.5) or (precision_0 == 0 or precision_1 == 0):\n",
    "        return 0 \n",
    "    imbalance_penalty = 1 - abs(precision_0 - precision_1)\n",
    "    return (precision_0 * precision_1 + max(precision_0, precision_1)) * imbalance_penalty\n",
    "custom_scorer = make_scorer(custom_precision_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Grid search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_reduction_configs = [\n",
    "    {\n",
    "        'dim_reduction': [None]\n",
    "    },\n",
    "    {\n",
    "        'dim_reduction': [PCA()],\n",
    "        'dim_reduction__n_components': [0.3, 0.5, 0.7, 0.9]\n",
    "    },\n",
    "    {\n",
    "        'dim_reduction': [LDA()],\n",
    "        'dim_reduction__n_components': [1]\n",
    "    },\n",
    "    {\n",
    "        'dim_reduction': [SFS(estimator=Perceptron(), cv=None, scoring=\"matthews_corrcoef\")],\n",
    "        'dim_reduction__estimator': [Perceptron(), LogisticRegression(), SVC()],\n",
    "        'dim_reduction__k_features': [10, 20, 30, 40, 50, 60, 70, 80]\n",
    "    },\n",
    "]\n",
    "\n",
    "classifier_configs = [\n",
    "    {\n",
    "        'classifier': [Perceptron()],\n",
    "        'classifier__eta0': loguniform(0.0001, 10),\n",
    "        'classifier__max_iter': [50, 100, 200, 500],\n",
    "        'classifier__class_weight': [None, 'balanced'],\n",
    "        'classifier__penalty': ['l1', 'l2', None]\n",
    "    },\n",
    "    {\n",
    "        'classifier': [LogisticRegression(solver='saga')],\n",
    "        'classifier__C': loguniform(0.0001, 10),\n",
    "        'classifier__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "        'classifier__class_weight': [None, 'balanced'],\n",
    "        'classifier__max_iter': [100, 500, 1000]\n",
    "    },\n",
    "    {\n",
    "        'classifier': [KNeighborsClassifier()],\n",
    "        'classifier__n_neighbors': [3, 5, 7, 9, 11, 13, 15, 17, 19, 21],\n",
    "        'classifier__weights': ['uniform', 'distance'],\n",
    "        'classifier__metric': ['euclidean', 'manhattan', 'chebyshev']\n",
    "    },\n",
    "    {\n",
    "        'classifier': [RandomForestClassifier()],\n",
    "        'classifier__n_estimators': [50, 100, 200, 500, 1000],\n",
    "        'classifier__max_depth': [None, 10, 20, 30, 50],\n",
    "        'classifier__min_samples_split': [2, 5, 10],\n",
    "        'classifier__min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    {\n",
    "        'classifier': [GradientBoostingClassifier()],\n",
    "        'classifier__n_estimators': [50, 100, 200, 500],\n",
    "        'classifier__learning_rate': loguniform(0.001, 0.1),\n",
    "        'classifier__max_depth': [3, 5, 7, 9],\n",
    "        'classifier__min_samples_split': [2, 5, 10],\n",
    "        'classifier__min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    {\n",
    "        'classifier': [XGBClassifier()],\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__learning_rate': loguniform(0.001, 0.1),\n",
    "        'classifier__max_depth': [3, 5, 7],\n",
    "        'classifier__subsample': [0.5, 0.7, 1.0],\n",
    "        'classifier__colsample_bytree': [0.5, 0.7, 1.0]\n",
    "    },\n",
    "    {\n",
    "        'classifier': [AdaBoostClassifier()],\n",
    "        'classifier__n_estimators': [50, 100, 200],\n",
    "        'classifier__learning_rate': loguniform(0.001, 1)\n",
    "    },\n",
    "    {\n",
    "        'classifier': [LinearSVC()],\n",
    "        'classifier__C': loguniform(0.001, 100),\n",
    "        'classifier__max_iter': [1000, 2000, 3000],\n",
    "        'classifier__class_weight': [None, 'balanced']\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_configs = []\n",
    "for configuration in itertools.product(dim_reduction_configs,classifier_configs):\n",
    "    all_parameters = []\n",
    "    for element in configuration:\n",
    "        for item in element.items():\n",
    "            all_parameters.append(item)\n",
    "    all_configs.append(dict(all_parameters)) \n",
    "print(len(all_configs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = RandomizedSearchCV(model_pipeline,\n",
    "    param_distributions=all_configs,\n",
    "    n_iter=len(all_configs) * 100,\n",
    "    n_jobs=-1,\n",
    "    cv = 5,\n",
    "    scoring='matthews_corrcoef'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(rs, X_train, y_train, scoring=\"matthews_corrcoef\", cv = 10, return_estimator=True, verbose=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, estimator in enumerate(scores['estimator']):\n",
    "    print(estimator.best_estimator_.get_params()['dim_reduction'])\n",
    "    print(estimator.best_estimator_.get_params()['classifier'],estimator.best_estimator_.get_params()['classifier'].get_params())\n",
    "    print(scores['test_score'][index])\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for estimator in scores['estimator']:\n",
    "    pred_train = estimator.best_estimator_.fit(X_train, y_train)\n",
    "    pred_train = estimator.best_estimator_.predict(X_train)\n",
    "    pred_test = estimator.best_estimator_.predict(X_test)\n",
    "    f1_train = f1_score(y_train, pred_train)\n",
    "    f1_test = f1_score(y_test, pred_test)\n",
    "    print(f'F1 on training set:{f1_train}, F1 on test set:{f1_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate result interpretation at a glance, I aim to have a comprehensive visual representation that allows for an immediate assessment of model performance.  \n",
    "To achieve this, I generate a single plot that displays all five confusion matrices in a unified view. This approach enables a quick and intuitive evaluation of classification performance across different conditions, making it easier to identify patterns, strengths, and potential weaknesses in the model's predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_estimators = len(scores['estimator'])\n",
    "nrows = 2\n",
    "ncols = 3\n",
    "\n",
    "fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "for index, estimator in enumerate(scores['estimator']):\n",
    "    pred_test = estimator.best_estimator_.predict(X_test)\n",
    "    report = classification_report(y_test, pred_test)\n",
    "    print(report)\n",
    "    cm = confusion_matrix(y_test, pred_test)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=estimator.classes_)\n",
    "    disp.plot(cmap='Blues', ax=axes[index], colorbar=False)\n",
    "    axes[index].set_title(f'Confusion Matrix for Estimator {index+1}')\n",
    "\n",
    "for ax in axes[num_estimators:]:\n",
    "    ax.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinement of the selected model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_pipeline_ref = IMBPipeline([\n",
    "    ('trans', preprocessor),\n",
    "    ('classifier', LogisticRegression(\n",
    "        C=1.944795783724781,\n",
    "        class_weight='balanced',\n",
    "        max_iter=1000,\n",
    "        penalty='l1',\n",
    "        solver='saga',\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "params = {\n",
    "    'classifier__C': np.logspace(np.log10(1.0), np.log10(3.0), num=5),\n",
    "    'classifier__penalty': ['l1', 'l2'],\n",
    "    'classifier__max_iter': [500, 1000, 1500]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_best = RandomizedSearchCV(\n",
    "    estimator = best_model_pipeline_ref,\n",
    "    param_distributions = params,\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1),\n",
    "    n_iter=20,\n",
    "    scoring='matthews_corrcoef'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_best.fit(X_train, y_train)\n",
    "cls = rs_best.best_estimator_\n",
    "y_pred= cls.predict(X_test)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "print(cls.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 Results Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Grid Search Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the experimentation phase, I tested various configurations, modifying the following parameters:  \n",
    "\n",
    "- **Stock selection**  \n",
    "- **Number of preceding candles** considered in each instance  \n",
    "- **Timeframe** (5 minutes, 10 minutes, 1 hour, 1 day)  \n",
    "\n",
    "The results revealed the following insights:  \n",
    "\n",
    "- Since the model relies solely on technical indicator analysis, without incorporating macroeconomic context or investor sentiment, **indices tend to perform better**. Individual stocks introduce excessive volatility, while, unexpectedly, currency pairs do not yield strong performance.  \n",
    "- **Optimal performance is achieved when considering at least 8-9 preceding candles**. However, beyond 12 candles, performance deteriorates.  \n",
    "- **The daily timeframe provides the best results** as it reduces noise. However, the difference compared to the 1 hour timeframe is not significant. Given similar performance, the 1 hour timeframe is preferable as it allows for a higher trade frequency, leading to potentially greater profits. \n",
    " \n",
    "I also decided to save the probabilities assigned to each choice of the model: 0 or 1, this could be very useful later as a threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Best model Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stock: Spy index\n",
    "#Timeframe: 1 hour\n",
    "\n",
    "best_model_pipeline_1 = IMBPipeline([\n",
    "    ('trans', preprocessor),\n",
    "    ('classifier', LinearSVC(C=2.0257272878736794))\n",
    "])\n",
    "best_model_pipeline_1.fit(X_train,y_train)\n",
    "y_pred_model_1 =  best_model_pipeline_1.predict(X_test)\n",
    "report = classification_report(y_test, y_pred_model_1)\n",
    "prediction_probabilities_model_1 = np.max(best_model_pipeline_1.predict_proba(X_test), axis=1)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After evaluating the performance of our best-performing model, the next step is to assess whether applying a basic buy/sell strategy based on the model's predicted labels can yield tangible results.\n",
    "\n",
    "Approach\n",
    "\n",
    "1. **Percentage Difference Calculation:**  \n",
    "   We calculate the absolute percentage difference between two consecutive candlesticks by comparing the current candlestick's (`Adj Close`) with that of the next one.\n",
    "\n",
    "2. **Normalization for Stop-Loss/Take-Profit:**  \n",
    "   The computed difference is normalized to 1%, effectively simulating a stop-loss/take-profit mechanism. This adjustment, while unnecessary for 1 hour candlesticks (except under extreme market conditions), proves useful when simulating daily candlesticks.\n",
    "\n",
    "3. **Utilizing Prediction Probability:**  \n",
    "   Tracking the prediction probability allows us to filter out low-confidence predictions:\n",
    "   - By setting a threshold, we can choose to perform more trades (with lower prediction probabilities) or opt for fewer, higher-quality trades.\n",
    "   - In real trading, favoring quality (i.e., higher prediction probability) is likely preferable, as it increases overall accuracy and reduces transaction costs.\n",
    "\n",
    ">**Practical example**   \n",
    ">without the filter, over the course of a month of testing, approximately 2000 trades are executed with an average accuracy of 54% and an average prediction probability of about 53%. However, if I decide not to consider those trades that are nearly random—i.e., with a prediction probability below 60%—the number of trades would drop to ~200/300, but the accuracy would soar to an impressive 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_analyze(X_test, y_test, y_pred, prediction_probabilitiess, model_n=1, prediction_probabilities_filter=0):\n",
    "    X_test[\"difference_abs_percentage\"] = abs(100 - X_test[\"Adj Close\"].shift(-1) * 100 / X_test[\"Adj Close\"])\n",
    "    X_test[\"label\"] = y_test\n",
    "    X_test[\"pred_label\"] = y_pred\n",
    "    X_test[\"prediction_probability\"] = prediction_probabilitiess  # Store prediction_probability level\n",
    "    X_test[\"Adj difference_abs_percentage\"] = X_test[\"difference_abs_percentage\"].apply(lambda x: 1 if x > 1 else -1 if x < -1 else x)\n",
    "    X_test.fillna(0.02, inplace=True)\n",
    "\n",
    "    Df = X_test[['Datetime', 'Adj Close', 'difference_abs_percentage', 'Adj difference_abs_percentage', \n",
    "                 'pred_label', 'label', 'prediction_probability']]\n",
    "    Df.to_csv(f\"output_model_{model_n}.csv\", index=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(X_test['Datetime'], X_test['Adj Close'], label='Adj Close')\n",
    "    plt.title('Adj Close Plot for One Day')\n",
    "    plt.xlabel('Datetime')\n",
    "    plt.ylabel('Adj Close')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    filtered_X_test = X_test[X_test[\"prediction_probability\"] > prediction_probabilities_filter]\n",
    "    \n",
    "    correct_labels = (filtered_X_test['label'] == filtered_X_test['pred_label']).sum()\n",
    "    wrong_labels = len(filtered_X_test) - correct_labels\n",
    "    avg_accuracy = correct_labels * 100 / (correct_labels + wrong_labels) if (correct_labels + wrong_labels) > 0 else 0\n",
    "\n",
    "    print(f\"Number of correct labels: {correct_labels}\")\n",
    "    print(f\"Number of wrong labels: {wrong_labels}\")\n",
    "    print(f\"Avg accuracy: {avg_accuracy:.2f}%\")\n",
    "    print(f\"Avg prediction_probability level: {np.mean(filtered_X_test['prediction_probability']) if not filtered_X_test.empty else 0:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_1= X_test\n",
    "y_test_model_1= y_test\n",
    "process_and_analyze(X_test_model_1, y_test_model_1, y_pred_model_1,prediction_probabilities_model_1,1, prediction_probabilities_filter=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function evaluates a high-leverage trading strategy that relies on predicted market movement signals. The strategy's performance is compared to a **Buy-and-Hold benchmark** and verified against key **FTMO Challenge criteria** to ensure it meets strict risk management and profitability standards.\n",
    "\n",
    "---\n",
    "**Buy/Short Sell Approach**  \n",
    "- **Signal-Based Trading**:  \n",
    "  For each candlestick, the strategy determines the trade direction based on the predicted label:  \n",
    "    - **Predicted Label = 1** → Go **Long** (buy the asset), expecting the price to rise.  \n",
    "    - **Predicted Label = 0** → Go **Short** (sell the asset), anticipating a price drop.  \n",
    "\n",
    "**Investment and Leverage**  \n",
    "- Each trade invests **2% of the current capital**.  \n",
    "- The strategy applies **100x leverage**, significantly amplifying both profits and losses.  \n",
    "\n",
    "**Prediction Filter**  \n",
    "- Trades are executed only if the **prediction probability** exceeds **60%** threshold, ensuring only high-confidence trades are taken.  \n",
    "- If the predicted label matches the actual market movement (`label`), the trade results in profit. Otherwise, it incurs a loss.\n",
    "---\n",
    "\n",
    "FTMO Challenge Specification  \n",
    "\n",
    "The function verifies the strategy against FTMO Challenge rules, simulating a real-world evaluation used by proprietary trading firms:  \n",
    "\n",
    "1. **Minimum Trading Days**:  \n",
    "   - The strategy must execute trades on at least **4 distinct trading days**.\n",
    "\n",
    "2. **Daily Loss Limit**:  \n",
    "   - No single day should result in a loss exceeding **5% of the initial capital**.  \n",
    "\n",
    "3. **Account Equity Maintenance**:  \n",
    "   - The account equity must remain above **90% of the initial capital** at all times.  \n",
    "\n",
    "4. **Profit Target**:  \n",
    "   - The strategy must achieve at least **5% in profit** over the evaluation period.\n",
    "\n",
    "> **Why Use FTMO Criteria?**  \n",
    "> These criteria help evaluate the robustness and risk management of the strategy under realistic constraints. Meeting these requirements demonstrates that the strategy can handle pressure, control losses, and remain profitable over time—essential traits for professional trading.  \n",
    "---\n",
    "\n",
    "Performance Comparison  \n",
    "\n",
    "The function benchmarks the trading strategy against a **Buy-and-Hold approach**:  \n",
    "- **Buy-and-Hold**: The initial capital is invested in the asset at the start of the period and held without trading.  \n",
    "- The function tracks the **capital progression** for both strategies and visualizes them for comparison.  \n",
    "\n",
    "---\n",
    "\n",
    "Performance Tracking  \n",
    "\n",
    "The function records and visualizes key metrics:  \n",
    "1. **Capital Evolution**: Comparison between the trading strategy and Buy-and-Hold over time.  \n",
    "2. **Maximum Drawdown**: The largest observed drop from a peak in trading capital.  \n",
    "3. **Consecutive Losses**: The maximum number of consecutive losing trades.  \n",
    "4. **Daily Profit and Loss (P&L)**: The accumulated profit or loss for each trading day.  \n",
    "5. **Monthly Error Distribution**: A bar chart showing the number of incorrect predictions (trading errors) per month.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_trading_strategy_and_verify(df, \n",
    "                                           initial_capital=10000, \n",
    "                                           investment_pct=0.02, \n",
    "                                           leverage=100, \n",
    "                                           min_trading_days=4,\n",
    "                                           max_daily_loss=500,         \n",
    "                                           min_account_equity_ratio=0.90, \n",
    "                                           profit_target_verification=500,\n",
    "                                           prediction_probability= 0\n",
    "                                          ):\n",
    "    \"\"\"\n",
    "    Evaluate a trading strategy against a Buy-and-Hold benchmark and verify performance against FTMO challenge rules, it includes a prediction_probability filter: trades happen only if prediction_probability > 55%.\n",
    "    \"\"\"\n",
    "    \n",
    "    required_columns = ['Datetime', 'Adj Close', 'difference_abs_percentage', 'label', 'pred_label', 'prediction_probability']\n",
    "    missing = [col for col in required_columns if col not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(\"DataFrame is missing required columns: \" + \", \".join(missing))\n",
    "    \n",
    "    df = df.rename(columns={'Adj Close': 'Adj_Close'})\n",
    "    \n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "    \n",
    "    capital = initial_capital\n",
    "    capital_history = [capital]\n",
    "    drawdowns = []\n",
    "    peak = capital\n",
    "\n",
    "    current_consecutive_errors = 0\n",
    "    max_consecutive_errors = 0\n",
    "    errors_by_month = defaultdict(int)\n",
    "\n",
    "    initial_price = df.iloc[0]['Adj_Close']\n",
    "    buy_and_hold_value = [initial_capital]\n",
    "    datetime_values = [df.iloc[0]['Datetime']]\n",
    "\n",
    "    daily_pnl = defaultdict(float)\n",
    "    trading_days = set()\n",
    "    account_stop_loss_breached = False\n",
    "\n",
    "    for row in df.itertuples(index=False):\n",
    "        if row.prediction_probability <= prediction_probability:\n",
    "            continue  \n",
    "\n",
    "        if pd.isna(row.label) or pd.isna(row.pred_label):\n",
    "            continue\n",
    "\n",
    "        trade_day = row.Datetime.date()\n",
    "        trade_month = row.Datetime.strftime(\"%Y-%m\")\n",
    "        trading_days.add(trade_day)\n",
    "\n",
    "        # Investment calculation\n",
    "        investment = capital * investment_pct\n",
    "        raw_trade_pl = investment * (row.difference_abs_percentage / 100) * leverage\n",
    "\n",
    "        # Determine profit or loss\n",
    "        pnl = abs(raw_trade_pl) if row.label == row.pred_label else -abs(raw_trade_pl)\n",
    "        capital += pnl\n",
    "\n",
    "        # Update error tracking\n",
    "        if pnl < 0:\n",
    "            current_consecutive_errors += 1\n",
    "            max_consecutive_errors = max(max_consecutive_errors, current_consecutive_errors)\n",
    "            errors_by_month[trade_month] += 1\n",
    "        else:\n",
    "            current_consecutive_errors = 0\n",
    "\n",
    "        # Record daily P&L\n",
    "        daily_pnl[trade_day] += pnl\n",
    "\n",
    "        # Update drawdown calculation\n",
    "        capital_history.append(capital)\n",
    "        peak = max(peak, capital)\n",
    "        drawdowns.append((peak - capital) / peak if peak > 0 else 0)\n",
    "\n",
    "        # Update Buy-and-Hold\n",
    "        buy_and_hold_value.append(initial_capital * (row.Adj_Close / initial_price))\n",
    "        datetime_values.append(row.Datetime)\n",
    "\n",
    "        # Check if account equity falls below threshold\n",
    "        if capital < initial_capital * min_account_equity_ratio:\n",
    "            account_stop_loss_breached = True\n",
    "\n",
    "    max_drawdown = max(drawdowns) if drawdowns else 0\n",
    "    worst_daily_pnl = min(daily_pnl.values()) if daily_pnl else 0\n",
    "    maximum_daily_loss_reached = abs(worst_daily_pnl)\n",
    "    maximum_daily_loss_percentage = (maximum_daily_loss_reached / initial_capital) * 100\n",
    "\n",
    "    # FTMO Challenge verification\n",
    "    num_trading_days = len(trading_days)\n",
    "    min_trading_days_met = num_trading_days >= min_trading_days\n",
    "    daily_loss_breaches = {day: pnl for day, pnl in daily_pnl.items() if pnl < -max_daily_loss}\n",
    "    max_daily_loss_met = (len(daily_loss_breaches) == 0)\n",
    "    account_stop_loss_met = not account_stop_loss_breached\n",
    "    profit_target_met = (capital - initial_capital) >= profit_target_verification\n",
    "\n",
    "    # Plot Performance vs. Buy & Hold\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(datetime_values, capital_history, label='Trading Strategy', color='blue')\n",
    "    plt.plot(datetime_values, buy_and_hold_value, label='Buy & Hold', color='green', linestyle='dashed')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Capital ($)')\n",
    "    plt.title('Trading Strategy vs. Buy & Hold Performance')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot errors per month\n",
    "    months = sorted(errors_by_month.keys())\n",
    "    errors = [errors_by_month[month] for month in months]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(months, errors, color='red')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Number of Errors')\n",
    "    plt.title('Distribution of Wrong Labels by Month')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Final Capital (Trading Strategy): ${capital:.2f}\")\n",
    "    print(f\"Final Capital (Buy & Hold): ${buy_and_hold_value[-1]:.2f}\")\n",
    "    print(f\"Max Drawdown: {max_drawdown * 100:.2f}%\")\n",
    "    print(f\"Maximum Consecutive Errors: {max_consecutive_errors}\")\n",
    "    print(f\"Max Daily Loss: ${maximum_daily_loss_reached:.2f}\")\n",
    "    print(f\"Max Daily Loss Percentage: {maximum_daily_loss_percentage:.2f}%\\n\")\n",
    "    \n",
    "    print(\"FTMO Challenge Verification Results:\")\n",
    "    print(f\" - Minimum Trading Days: {num_trading_days} (Required: {min_trading_days}) --> {'PASSED' if min_trading_days_met else 'FAILED'}\")\n",
    "    if daily_loss_breaches:\n",
    "        for day, pnl in daily_loss_breaches.items():\n",
    "            print(f\" - Day {day} exceeded max daily loss: Loss = ${-pnl:.2f} (Limit: ${max_daily_loss})\")\n",
    "    print(f\" - Max Daily Loss Rule: {'PASSED' if max_daily_loss_met else 'FAILED'}\")\n",
    "    print(f\" - Account Stop-Loss: {'PASSED' if account_stop_loss_met else 'FAILED'}\")\n",
    "    print(f\" - Profit Target: {'PASSED' if profit_target_met else 'FAILED'}\")\n",
    "    \n",
    "    return {\n",
    "        'final_capital': capital,\n",
    "        'buy_and_hold_final': buy_and_hold_value[-1],\n",
    "        'max_drawdown': max_drawdown,\n",
    "        'max_consecutive_errors': max_consecutive_errors,\n",
    "        'num_trading_days': num_trading_days,\n",
    "        'daily_loss_breaches': daily_loss_breaches,\n",
    "        'account_stop_loss_met': account_stop_loss_met,\n",
    "        'profit_target_met': profit_target_met,\n",
    "        'maximum_daily_loss_reached': maximum_daily_loss_reached,\n",
    "        'maximum_daily_loss_percentage': maximum_daily_loss_percentage,\n",
    "        'errors_by_month': dict(errors_by_month)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1=pd.read_csv(\"output_model_1.csv\")\n",
    "evaluate_trading_strategy_and_verify(df_1,prediction_probability=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Second Best model Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the results presented using the same approach as in the previous paragraph, but with a different decision model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stock: Spy index\n",
    "#Timeframe: 5min \n",
    "best_model_pipeline_2 = IMBPipeline([\n",
    "    ('trans', preprocessor),\n",
    "    ('classifier', LogisticRegression(C=0.784907138175208, class_weight='balanced', max_iter=500, penalty='l1', solver='saga'))\n",
    "])\n",
    "best_model_pipeline_2.fit(X_train,y_train)\n",
    "y_pred_model_2= best_model_pipeline_2.predict(X_test)\n",
    "report = classification_report(y_test, y_pred_model_2)\n",
    "prediction_probabilities_model_2 = np.max((best_model_pipeline_2.predict_proba(X_test)), axis=1)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_2= X_test\n",
    "y_test_model_2= y_test\n",
    "process_and_analyze(X_test_model_2, y_test_model_2, y_pred_model_2,prediction_probabilities_model_2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_trading_strategy_and_verify(X_test_model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Third Best model Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the results presented using the same approach as in the previous paragraph, but with a different decision model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stock: Spy index\n",
    "#Timeframe: 1Hour \n",
    "best_model_pipeline_3 = IMBPipeline([\n",
    "    ('trans', preprocessor),\n",
    "    ('classifier', LinearSVC(C=2.0257272878736794))\n",
    "])\n",
    "best_model_pipeline_3.fit(X_train, y_train)\n",
    "y_pred_model_3 = best_model_pipeline_3.predict(X_test)\n",
    "report = classification_report(y_test, y_pred_model_3)\n",
    "prediction_probabilities_model_3 = [0.7] * len(X_test)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_model_3= X_test\n",
    "y_test_model_3= y_test\n",
    "process_and_analyze(X_test_model_3, y_test_model_3, y_pred_model_3,prediction_probabilities_model_3,3,prediction_probabilities_filter=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_trading_strategy_and_verify(X_test_model_3,prediction_probability=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Learning visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes, train_scores, test_scores = learning_curve(best_model_pipeline_1,\n",
    "                                                       X=X_train,\n",
    "                                                       y=y_train,\n",
    "                                                       train_sizes= [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                                                       cv = 5,\n",
    "                                                       n_jobs = -1,\n",
    "                                                       scoring = 'matthews_corrcoef',\n",
    "                                                       shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curve Plot\n",
    "\n",
    "This code visualizes training and validation Matthews Correlation Coefficient (MCC) to assess model performance across different training set sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "fig=plt.figure(figsize=(12,7))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "ax.plot(train_sizes, train_mean,\n",
    "         color='blue', marker='+',\n",
    "         markersize=5, label='Training MCC')\n",
    "\n",
    "ax.fill_between(train_sizes,\n",
    "                 train_mean + train_std,\n",
    "                 train_mean - train_std,\n",
    "                 alpha=0.15, color='blue')\n",
    "\n",
    "ax.plot(train_sizes, test_mean,\n",
    "         color='green', linestyle='--',\n",
    "         marker='d', markersize=5,\n",
    "         label='Validation MCC')\n",
    "\n",
    "ax.fill_between(train_sizes,\n",
    "                 test_mean + test_std,\n",
    "                 test_mean - test_std,\n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "ax.grid()\n",
    "ax.set_xlabel('Training set size')\n",
    "ax.set_ylabel('matthews_corrcoef')\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_ylim([-0.05, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_C = [0.001,0.01,0.1,1,10,100]\n",
    "train_scores, test_scores = validation_curve(best_model_pipeline_1,\n",
    "        X=X_train, \n",
    "        y=y_train, \n",
    "        param_range=\n",
    "        range_C, \n",
    "        param_name='classifier__C',\n",
    "        cv=5, \n",
    "        n_jobs=-1, \n",
    "        scoring='matthews_corrcoef'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Curve for Parameter C  \n",
    "This code plots the validation curve for different values of `C`, analyzing its effect on training and validation Matthews Correlation Coefficient (MCC), with shaded regions indicating variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "fig=plt.figure(figsize=(12,7))\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(range_C, train_mean,\n",
    "         color='blue', marker='o',\n",
    "         markersize=5, label='Training MCC')\n",
    "\n",
    "ax.fill_between(range_C,\n",
    "                 train_mean + train_std,\n",
    "                 train_mean - train_std,\n",
    "                 alpha=0.15, color='blue')\n",
    "\n",
    "ax.plot(range_C, test_mean,\n",
    "         color='green', linestyle='--',\n",
    "         marker='s', markersize=5,\n",
    "         label='Validation MCC')\n",
    "\n",
    "ax.fill_between(range_C,\n",
    "                 test_mean + test_std,\n",
    "                 test_mean - test_std,\n",
    "                 alpha=0.15, color='green')\n",
    "\n",
    "ax.grid()\n",
    "ax.set_xlabel('Parameter C')\n",
    "ax.set_ylabel('matthews_corrcoef')\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_ylim([-0.05, 0.15])\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim([0.05,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall Curve  \n",
    "By plotting **Precision and Recall vs. Threshold**, I can better understand how different scores affect model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = best_model_pipeline_1.decision_function(X_train)\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train, scores)\n",
    "threshold = 0.2\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", lw=2)\n",
    "ax.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", lw=2)\n",
    "ax.vlines(threshold, 0, 1.0, \"k\", \"dotted\", label=\"threshold\")\n",
    "\n",
    "idx = (thresholds >= threshold).argmax()  # first index ≥ threshold\n",
    "plt.plot(thresholds[idx], precisions[idx], \"bo\")\n",
    "plt.plot(thresholds[idx], recalls[idx], \"go\")\n",
    "plt.grid()\n",
    "ax.set_xlabel(\"Threshold\")\n",
    "ax.set_xlim((-0.85,1.5))\n",
    "plt.legend(loc=\"center right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot()\n",
    "ax.plot(recalls, precisions, lw=2, label=\"Precision/Recall curve\")\n",
    "ax.plot([recalls[idx], recalls[idx]], [0., precisions[idx]], \"k:\")\n",
    "ax.plot([0.0, recalls[idx]], [precisions[idx], precisions[idx]], \"k:\")\n",
    "ax.plot([recalls[idx]], [precisions[idx]], \"ko\",\n",
    "         label=f\"Point at threshold {threshold}\")\n",
    "ax.set_xlabel(\"Recall\")\n",
    "ax.set_ylabel(\"Precision\")\n",
    "ax.axis([0, 1, 0, 1])\n",
    "ax.legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve  \n",
    "\n",
    "Evaluates model performance by showing the trade-off between False Positive Rate and Recall across thresholds, with AUC summarizing overall discrimination ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "fprs, recalls, thresholds = roc_curve(y_train,scores) \n",
    "fig = plt.figure(figsize=(6, 5)) \n",
    "ax = fig.add_subplot()\n",
    "ax.plot(fprs, recalls, linewidth=2, label=\"ROC curve\")\n",
    "ax.plot([0, 1], [0, 1], 'k:', label=\"Random classifier's ROC curve\")\n",
    "ax.set_xlabel('False Positive Rate - FPR')\n",
    "ax.set_ylabel('Recall')\n",
    "ax.axis([0, 1, 0, 1])\n",
    "ax.legend(loc=\"lower right\", fontsize=13)\n",
    "\n",
    "roc_auc_score(y_train, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Financial Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selected machine learning classification models demonstrated strong performance, with the following algorithms achieving the best results:\n",
    "- **Logistic Regression**\n",
    "- **Support Vector Classifier (SVC)**\n",
    "\n",
    "**Feature Reduction**:\n",
    "During the experimentation phase, an attempt was made to enforce feature reduction prior to grid search, retaining only the top 70 features deemed most useful by the model. However, this approach did not yield significantly different results compared to a model trained on the full feature set. The primary benefit of this a priori feature reduction was a substantial decrease in grid search computation time.\n",
    "\n",
    "**Timeframe Analysis**:\n",
    "The models performed better when analyzing longer timeframes, such as **1-day candles**, though the difference was not drastic:\n",
    "- **1-Day Timeframe**: Average accuracy ~ **60%**\n",
    "- **1-Hour Timeframe**: Average accuracy ~ **55%**\n",
    "\n",
    "While the 1-Hour timeframe allows for more trades within the same period compared to the 1-day timeframe, the accuracy trade-off must be considered.\n",
    "\n",
    "**Index vs. Single Stock Performance**:\n",
    "The models exhibited superior performance when analyzing indices, such as the **S&P 500**, which includes the top 500 companies by market capitalization, compared to individual stocks. This is likely due to the current reliance on technical indicators alone, without incorporating news-based analysis, which significantly impacts individual stock performance.\n",
    "\n",
    "**Investment Strategy**:\n",
    "A simple investment strategy was employed:\n",
    "- **Buy** the stock if the predicted label is **1**.\n",
    "- **Short sell** the stock if the predicted label is **0**.\n",
    "\n",
    "For this example, a **100x leverage** (highly risky and not recommended for non-experts) and **1% of available capital per trade** were used. No stop-loss or take-profit mechanisms were applied, as the 1-Hour timeframe's volatility allowed trades to be closed at the end of the candle.\n",
    "\n",
    "**Test Period Results**:\n",
    "Using the above setup on a **6-month test period** with an initial capital of **$10,000**, trading daily on the S&P 500 with 1-Hour candles, the model achieved the following results :\n",
    "- **Final Capital**: ~$15,000–$16,000\n",
    "- **Capital Increase**: ~5–60%\n",
    "- **Maximum Drawdown**: ~9–11%\n",
    "- **Maximum Consecutive Errors**: 4–8 times\n",
    "- **Maximum Daily Loss Percentage**: ~4–4%\n",
    "- **Accuracy (Correct Trades)**: ~53–58% (not considering the improvement brought by using probabilities as a filter)\n",
    "\n",
    "**Limitations**:\n",
    "As demonstrated in Section 3.1, applying an investment strategy based on machine learning decision models is feasible and has shown promising results. However, several critical limitations must be acknowledged:\n",
    "1. **Leverage Dependency**: The impressive results are largely attributable to the use of **financial leverage**, a powerful tool that can lead to significant losses if not managed properly (e.g., through stop-loss mechanisms). Without leverage, the algorithm's performance would be substantially weaker.\n",
    "2. **Data Source Limitations**: The model was trained using data from two separate sources due to limitations in data availability. Yahoo Finance restricts historical data to the past month, while Alpaca API does not provide live data access.\n",
    "3. **Future Performance Uncertainty**: While the model has performed well historically, there is no guarantee of continued success. Continuous retraining and monitoring are essential to maintain performance.\n",
    "\n",
    "**Disclaimer**\n",
    "The information presented in this notebook is **not financial advice**. Relying solely on a machine learning model for investment decisions is not a sound strategy and should not be considered without thorough research and professional guidance. Always conduct your own due diligence before making any investment decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 Future Improvements and references. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Upgrades\n",
    "\n",
    "The following improvements are proposed to enhance the robustness and practicality of the model:\n",
    "\n",
    "1. **Generalization Across Stocks**  \n",
    "   The predictor can be tested on a broader range of stocks to evaluate its robustness and adaptability. The goal is to develop a more generalized predictor capable of performing well across diverse stock market conditions.\n",
    "\n",
    "2. **Portfolio Construction and Risk Diversification**  \n",
    "   A portfolio comprising multiple stocks can be constructed to diversify risk and improve overall performance. Additionally, transaction costs should be incorporated into the evaluation framework to ensure the strategy’s effectiveness reflects real-world trading conditions.\n",
    "\n",
    "3. **Real Trading Environment Backtesting**  \n",
    "   Rigorous backtesting of the model in a simulated real trading environment can be conducted to assess its performance under realistic market dynamics and constraints.\n",
    "\n",
    "4. **Stop-Loss and Take-Profit Strategies**  \n",
    "   Robust stop-loss and take-profit mechanisms can be developed to mitigate losses and lock in gains, ensuring the model operates within predefined risk management parameters.\n",
    "\n",
    "5. **Data Continuity with a Single Data Source**  \n",
    "   Using a single, consistent data source ensures continuity and reduces discrepancies caused by different data providers. This improvement enhances the model’s stability and reliability during training and evaluation.\n",
    "\n",
    "6. **Incorporating Additional Technical Features**  \n",
    "   Expanding the feature set with more technical indicators\n",
    "\n",
    "7. **Incorporating Additional Non-Technical Features**  \n",
    "   Integrating sentiment analysis from financial news, social media, and earnings reports can provide valuable insights into market sentiment. Natural Language Processing (NLP) techniques can be used to analyze textual data and extract relevant signals that may impact stock movements.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Chen, Y., & Jiang, S. (2015). Stock market forecasting using machine learning algorithms. Stanford University, CS229 Project Report. Retrieved from https://cs229.stanford.edu/proj2015/009_report.pdf\n",
    "\n",
    "- Dai, X., & Zhang, Y. (2013). Machine learning in stock price trend forecasting. Stanford University, CS229 Project Report. Retrieved from https://cs229.stanford.edu/proj2013/DaiZhang-MachineLearningInStockPriceTrendForecasting.pdf\n",
    "\n",
    "- Shen, J., Jiang, S., & Zhang, Y. (2012). Stock market forecasting using machine learning algorithms. Stanford University, CS229 Project Report. Retrieved from https://cs229.stanford.edu/proj2012/ShenJiangZhang-StockMarketForecastingusingMachineLearningAlgorithms.pdf\n",
    "\n",
    "- Scorpionhiccup. (n.d.). Stock price prediction. GitHub repository. Retrieved from https://github.com/scorpionhiccup/StockPricePrediction?tab=readme-ov-file\n",
    "\n",
    "- Scikit-learn developers. (n.d.). Scikit-learn: Machine learning in Python. Retrieved from http://scikit-learn.org/\n",
    "\n",
    "- StockCharts.com. (n.d.). Chart school. Retrieved from http://stockcharts.com/school/doku.php?id=chart_school\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is licensed under the Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0).  \n",
    "It allows others to:  \n",
    "- **Share** — copy and redistribute the material in any medium or format  \n",
    "- **Adapt** — remix, transform, and build upon the material  \n",
    "\n",
    "Under the following conditions:  \n",
    "- **Attribution** — Appropriate credit must be given, a link to the license provided, and any changes indicated.  \n",
    "- **NonCommercial** — The material cannot be used for commercial purposes.  \n",
    "\n",
    "For more information, visit the full license at [https://creativecommons.org/licenses/by-nc/4.0/](https://creativecommons.org/licenses/by-nc/4.0/).\n"
   ]
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "1687e005e1f84cfa90ba1cf16a9f4e24",
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
